{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shread train-dev-test csv datasets progressively using BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max_farmhash  count\n",
      "0         None      0\n",
      "   min_farmhash10  max_farmhash10  count\n",
      "0             100             150  10333\n"
     ]
    }
   ],
   "source": [
    "def test_sample(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MAX(farmhash) as max_farmhash, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (\n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)  as farmhash, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2} )\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           a, b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "EVERY_N = 100\n",
    "query_maxhash = test_sample(0,70).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_maxhash = bq.Query(query_maxhash).execute().result().to_dataframe()\n",
    "print(df_maxhash)\n",
    "\n",
    "\n",
    "def test_sample2(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MIN(farmhash10) as min_farmhash10, MAX(farmhash10) as max_farmhash10, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (  \n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)*10  as farmhash10, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2})\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (10*10)+a, (10*10)+b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "  #return \"{}\\n{}\".format(basequery, sampler)\n",
    "\n",
    "\n",
    "EVERY_N = 100\n",
    "queryhash = test_sample2(0,60).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_hash = bq.Query(queryhash).execute().result().to_dataframe()\n",
    "print(df_hash.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "def sample_between(a, b, shredstart):\n",
    "  basequery = \"\"\"\n",
    "  SELECT \n",
    "    answer_count, comment_count, favorite_count,  score, view_count,\n",
    "    TIMESTAMP_DIFF(last_activity_date, creation_date, DAY) as days_posted,\n",
    "    IF(accepted_answer_id IS NULL , 0, 1 ) as accepted\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  \n",
    "  # Use sampling for initial model development. Once model is developed, shread the entire dataset into  .csv files based on condition in the sampler.\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < {1} AND MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= {0}\".format(\n",
    "            shredstart, shredstart + 10\n",
    "            )\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2}\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (shredstart*10)+a, (shredstart*10)+b\n",
    "          )\n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "\n",
    "def create_query(phase, EVERY_N, shredstart):\n",
    "  \"\"\"Phase: train (70%) valid (15%) or test (15%)\"\"\"\n",
    "  query = \"\"\n",
    "  if phase == 'train':\n",
    "    query = sample_between(0,60, shredstart)\n",
    "  elif phase == 'valid':\n",
    "    query = sample_between(60,75, shredstart)\n",
    "  else:\n",
    "    query = sample_between(75, 100, shredstart)\n",
    "  return query.replace(\"EVERY_N\", str(EVERY_N))\n",
    "\n",
    "#print(create_query('train', 100))\n",
    "#(answer_count - AVG(answer_count)) / STDDEV_POP(answer_count)  as answer_count,\n",
    "#IF(accepted_answer_id IS NULL , cast(0 as int64), cast(1 as int64)) as accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "def to_csv(df, filename):\n",
    "  outdf = df.copy(deep = True)\n",
    "  #outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # Reorder columns so that target is first column\n",
    "  #print(outdf.head())\n",
    "  #print(df.head())\n",
    "  cols = outdf.columns.tolist()\n",
    "  #print(cols)\n",
    "  cols.remove('accepted')\n",
    "  cols.insert(0, 'accepted')\n",
    "  #print(cols)\n",
    "  outdf = outdf[cols]  \n",
    "  \n",
    "  \n",
    "  #Normalizing input columns  and replace NaN or null\n",
    "  normalize_cols = outdf.columns.tolist()\n",
    "  normalize_cols.remove('accepted')\n",
    "  for normalize_cols_name in normalize_cols:\n",
    "    outdf[normalize_cols_name].fillna(0, inplace = True)\n",
    "    outdf[normalize_cols_name] = (outdf[normalize_cols_name] - outdf[normalize_cols_name].mean())  / outdf[normalize_cols_name].std() \n",
    "  #print(outdf)\n",
    "  #print(outdf['answer_count'] )\n",
    "  outdf.to_csv(filename,  header = False, index_label = False, index = False)\n",
    "  print(\"Wrote {} to {}\".format(len(outdf), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "  #for x in range(2):\n",
    "  for x in range(10):\n",
    "    query = create_query(phase, 100, x*10)\n",
    "    #print(query)\n",
    "    df = bq.Query(query).execute().result().to_dataframe()\n",
    "    #print(df.head())\n",
    "    to_csv(df, 'stackoverflow-{}-{}.csv'.format(phase,(x+1)*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refactor by not using sampling and creating large shreaded datasets (check size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "head stackoverflow-test-100.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before submitting training job using gcloud, first copy the training data to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clear Cloud Storage bucket and copy the CSV files to Cloud Storage bucket\n",
    "echo $BUCKET\n",
    "gsutil -m rm -rf gs://${BUCKET}/${MODEL_NAME}/smallinput/\n",
    "gsutil -m cp ${PWD}/*.csv gs://${BUCKET}/${MODEL_NAME}/smallinput/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that we have TensorFlow 1.13.1 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "\n",
    "#tf.estimator modeling\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from google.datalab.ml import TensorBoard\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  420953 May 16 19:56 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  410883 May 16 19:55 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  430781 May 16 19:55 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427660 May 16 19:55 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418436 May 16 19:55 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  441018 May 16 19:55 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  425796 May 16 19:55 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429537 May 16 19:55 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  443612 May 16 19:55 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427058 May 16 19:56 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1276647 May 16 19:54 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272196 May 16 19:52 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1268241 May 16 19:52 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1304954 May 16 19:52 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280210 May 16 19:53 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1278970 May 16 19:53 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1286367 May 16 19:53 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1283776 May 16 19:53 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1266617 May 16 19:53 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1283209 May 16 19:53 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  418194 May 16 19:55 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431702 May 16 19:54 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  428810 May 16 19:54 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413285 May 16 19:54 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  442332 May 16 19:54 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  427866 May 16 19:54 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  425989 May 16 19:54 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423116 May 16 19:54 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  434830 May 16 19:54 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431025 May 16 19:54 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Before reading csv was incorporated using pandas dataframe\n",
    "# But now reading csv is incorporated using tensorflow and so it's in the graps and also it reads progressively the shreaded files\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"./stackoverflow-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./stackoverflow-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./stackoverflow-test.csv\")\n",
    "\n",
    "CSV_COLUMNNAMES = list(df_train) # CSV_COLUMNNAMES = df_train.columns.tolist()\n",
    "print(CSV_COLUMNNAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMNNAMES[1:]\n",
    "LABEL_NAME = CSV_COLUMNNAMES[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10332 entries, 0 to 10331\n",
      "Data columns (total 7 columns):\n",
      "0                       10332 non-null int64\n",
      "9.17859509730161        10332 non-null float64\n",
      "0.016391595864342398    10332 non-null float64\n",
      "6.613646539294375       10332 non-null float64\n",
      "10.451072671114654      10332 non-null float64\n",
      "4.223676425737634       10332 non-null float64\n",
      "6.5386884140305765      10332 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 565.1 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3351 entries, 0 to 3350\n",
      "Data columns (total 7 columns):\n",
      "1                       3351 non-null int64\n",
      "0.12470310007233937     3351 non-null float64\n",
      "2.2582277091259537      3351 non-null float64\n",
      "-0.0656459229647584     3351 non-null float64\n",
      "-0.18404444736230954    3351 non-null float64\n",
      "-0.14508480486243067    3351 non-null float64\n",
      "-0.3741223082495156     3351 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 183.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Debugging  issue:  Field 0 in record 0 is not a valid int32: accepted\n",
    "#\t [[{{node DecodeCSV}}]]\n",
    "#\t [[node IteratorGetNext (defined at /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/util.py:110) ]]\n",
    "\n",
    "# Solution :  Skip the first line (header row) \n",
    "# skip(count)  :  Creates a Dataset that skips count elements from this dataset.\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"stackoverflow-train-20.csv\")\n",
    "df_train.info()\n",
    "\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"stackoverflow-test-10.csv\")\n",
    "df_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "CSV_COLUMNS = ['accepted', 'answer_count', 'comment_count', 'favorite_count', 'score', 'view_count', 'days_posted']\n",
    "DEFAULTS = [[0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "#DEFAULTS = [tf.constant([0], dtype=tf.int32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#           tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32) ]\n",
    "\n",
    "#i=0\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def decode_line(row):\n",
    "    #print(row)\n",
    "    cols = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    #print(cols)\n",
    "    features = dict(zip(CSV_COLUMNS,cols))\n",
    "    #print(i+1)\n",
    "    label = features.pop('accepted')  # remove label from features and store\n",
    "    #print(\"features: {} \\n label: {}\".format(features, label))\n",
    "    return features, label\n",
    "  \n",
    "  # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "  # Read lines from text files\n",
    "  #textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset).skip(1)\n",
    "  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "  # Parse text lines as comma-separated values (CSV)\n",
    "  dataset = textlines_dataset.map(decode_line)\n",
    "  \n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "  \n",
    "  if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "    num_epochs = 10  # loop indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size = 10*batch_size, seed=2)\n",
    "  else:\n",
    "    num_epochs = 1\n",
    "  \n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "  \n",
    "def get_train_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-train-*.csv', tf.estimator.ModeKeys.TRAIN)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  #print(\"Training set :  \\nfeatures1 : {}\\nlabel: {}\".format(features1, label1))\n",
    "  with tf.Session() as sess:\n",
    "    print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "  return features1, label1 \n",
    "\n",
    "def get_valid_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-valid-*.csv', tf.estimator.ModeKeys.EVAL)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  return features1, label1 \n",
    "\n",
    "def get_test_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-test-*.csv', tf.estimator.ModeKeys.PREDICT)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  with tf.Session() as sess:\n",
    "    print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "  return features1, label1 \n",
    "\n",
    "\n",
    "#get_train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMNS[1:]\n",
    "LABEL_NAME = CSV_COLUMNS[0]\n",
    "\n",
    "featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refactored for Distributed training and Monitoring \n",
    "'''\n",
    "%%time\n",
    "OUTDIR = \"stackoverflow_model\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)  \n",
    "  )\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda : get_train_input_fn()\n",
    "    #,steps = 200\n",
    "  )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refactored for Distributed training and Monitoring \n",
    "\n",
    "'''\n",
    "def validate_rmse(model):\n",
    "  metrices = model.evaluate(input_fn = lambda : get_valid_input_fn() )\n",
    "  print(\"RMSE on dataset = {}\".format(metrices[\"average_loss\"]**.5))\n",
    "\n",
    "#validate_rmse(model, df_train)\n",
    "validate_rmse(model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving input function\n",
    "Defines the expected shape of the JSON feed that the model will receive once deployed behind a REST API in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "  \n",
    "  json_features_placeholder = {\n",
    "    'answer_count' : tf.placeholder(tf.float32, [None]), #Batch size\n",
    "    'comment_count' : tf.placeholder(tf.float32, [None]), \n",
    "    'favorite_count' : tf.placeholder(tf.float32, [None]), \n",
    "    'score' : tf.placeholder(tf.float32, [None]), \n",
    "    'view_count' : tf.placeholder(tf.float32, [None]),  \n",
    "    'days_posted' : tf.placeholder(tf.float32, [None])\n",
    "  }\n",
    "  \n",
    "  features = json_features_placeholder\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(features, json_features_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.estimator.train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile --append ./stackoverflow/trainer/model.py\n",
    "\n",
    "\n",
    "## Create train and evaluate function using tf.estimator\n",
    "def train_and_evaluate(outputdir, num_train_steps):\n",
    "\n",
    "  run_config = tf.estimator.RunConfig(model_dir = outputdir, save_summary_steps = 100, save_checkpoints_steps = 1000)\n",
    "  \n",
    "  estimator = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    #model_dir = OUTDIR,\n",
    "    config = run_config \n",
    "  )\n",
    "  \n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = lambda : get_train_input_fn(), max_steps = num_train_steps)\n",
    "  \n",
    "  exporter_latest =  tf.estimator.LatestExporter('exporter', serving_input_receiver_fn = serving_input_fn)\n",
    "  \n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = lambda : get_valid_input_fn(), \n",
    "                                   steps = None,\n",
    "                                   start_delay_secs = 1, # start evaluating after N seconds\n",
    "                                   throttle_secs = 10,   # evaluate every N seconds\n",
    "                                   exporters = exporter_latest)\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring with TensorBoard\n",
    "Use \"refresh\" in Tensorboard during training to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 8021. Click <a href=\"/_proxy/58491/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8021"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTDIR = \"stackoverflow_model\"\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "train_and_evaluate(OUTDIR, num_train_steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "validate_rmse for 10 epochs on 2 validate csvs:\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 401: accuracy = 0.67553574, accuracy_baseline = 0.5273982, auc = 0.73186123, auc_precision_recall = 0.7124213, average_loss = 0.5643283, global_step = 401, label/mean = 0.5273982, loss = 280.26962, precision = 0.6314025, prediction/mean = 0.53914005, recall = 0.9244614\n",
    "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 401: stackoverflow_model/model.ckpt-401\n",
    "RMSE on dataset = 0.751217886417676\n",
    "\n",
    "\n",
    "\n",
    "on training set evaluate\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.6811679, accuracy_baseline = 0.5281519, auc = 0.7331483, auc_precision_recall = 0.7117538, average_loss = 0.56418276, global_step = 500, label/mean = 0.5281519, loss = 71.741234, precision = 0.6524123, prediction/mean = 0.48095724, recall = 0.8482496\n",
    "RMSE on dataset = 0.7511210011251841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now shut Tensorboard down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logdir</th>\n",
       "      <th>pid</th>\n",
       "      <th>port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>taxi_trained</td>\n",
       "      <td>3862</td>\n",
       "      <td>40839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taxi_trained</td>\n",
       "      <td>3956</td>\n",
       "      <td>33539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stackoverflow_model</td>\n",
       "      <td>8021</td>\n",
       "      <td>58491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                logdir   pid   port\n",
       "0         taxi_trained  3862  40839\n",
       "1         taxi_trained  3956  33539\n",
       "2  stackoverflow_model  8021  58491"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to list Tensorboard instances\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop TensorBoard fill the correct pid below\n",
    "TensorBoard().stop(33539)\n",
    "print(\"Stopped Tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-2013\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[34581]\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model.predict( input_fn = lambda : get_test_input_fn() )\n",
    "\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "# class_ids determine the prediction\n",
    "\n",
    "predictions = [p['class_ids'][0] for p in raw_predictions]\n",
    "#confusion_matrix = tf.confusion_matrix(df_test['accepted'], predictions)\n",
    "#print(confusion_matrix)\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run(tf.shape(predictions))) # output: [ 0.42116176  0.40666069]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_one_shot_iterator()  \n",
    "\n",
    "##### Parse all the *.csv files using make_one_shot_iterator()  \n",
    "\n",
    "##### How it works:\n",
    "\n",
    "Suppose we have only 2.csv files having #entries 10185 and 10332\n",
    "\n",
    "Total number of iterations when make_one_shot_iterator() is used:   401\n",
    "\n",
    "\n",
    "= ( 10185*10 + 10332*10 ) / 512\n",
    "\n",
    "= ( 101850 + 103320 ) / 512 =  400.72265625\n",
    "\n",
    "where \n",
    "  - 10 = 10 epochs\n",
    "  - 512 = batch size\n",
    "  - 10185*10  = 1st csv total no of entries to be iterated over\n",
    "  - 10332*10  = 2st csv total no of entries to be iterated over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-48e40be9b234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accepted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test_predictions = df_test.copy(deep = True)\n",
    "df_test_predictions['accepted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(df_test, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_test_predictions, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
