{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "mkdir -p stackoverflow/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./stackoverflow/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%writefile ./stackoverflow/trainer/model.py\n",
    "#tf.estimator modeling\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "CSV_COLUMNS = ['accepted', 'answer_count', 'comment_count', 'favorite_count', 'score', 'view_count', 'days_posted']\n",
    "DEFAULTS = [[0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "#DEFAULTS = [tf.constant([0], dtype=tf.int32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#           tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32) ]\n",
    "\n",
    "#i=0\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def decode_line(row):\n",
    "        #print(row)\n",
    "        cols = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "        #print(cols)\n",
    "        features = dict(zip(CSV_COLUMNS,cols))\n",
    "        #print(i+1)\n",
    "        label = features.pop('accepted')  # remove label from features and store\n",
    "        #print(\"features: {} \\n label: {}\".format(features, label))\n",
    "        return features, label\n",
    "  \n",
    "    # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "    # Read lines from text files\n",
    "    #textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset).skip(1)\n",
    "    textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "    # Parse text lines as comma-separated values (CSV)\n",
    "    dataset = textlines_dataset.map(decode_line)\n",
    "  \n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "  \n",
    "    if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "        num_epochs = 10  # loop indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 10*batch_size, seed=2)\n",
    "    else:\n",
    "        num_epochs = 1\n",
    "  \n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "  \n",
    "def get_train_input_fn(folder_name):\n",
    "    dataset = read_dataset(folder_name + '/stackoverflow-train-*.csv', tf.estimator.ModeKeys.TRAIN)\n",
    "    features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "    #print(\"Training set :  \\nfeatures1 : {}\\nlabel: {}\".format(features1, label1))\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "    return features1, label1 \n",
    "\n",
    "def get_valid_input_fn(folder_name):\n",
    "    dataset = read_dataset(folder_name + '/stackoverflow-valid-*.csv', tf.estimator.ModeKeys.EVAL)\n",
    "    features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "    return features1, label1 \n",
    "\n",
    "def get_test_input_fn(folder_name):\n",
    "    dataset = read_dataset(folder_name + '/stackoverflow-test-*.csv', tf.estimator.ModeKeys.PREDICT)\n",
    "    features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "    return features1, label1 \n",
    "\n",
    "\n",
    "#get_train_input_fn()\n",
    "FEATURE_NAMES = CSV_COLUMNS[1:]\n",
    "LABEL_NAME = CSV_COLUMNS[0]\n",
    "\n",
    "featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols)\n",
    "\n",
    "def serving_input_fn():\n",
    "  \n",
    "    json_features_placeholder = {\n",
    "        'answer_count' : tf.placeholder(tf.float32, [None]), #Batch size\n",
    "        'comment_count' : tf.placeholder(tf.float32, [None]), \n",
    "        'favorite_count' : tf.placeholder(tf.float32, [None]), \n",
    "        'score' : tf.placeholder(tf.float32, [None]), \n",
    "        'view_count' : tf.placeholder(tf.float32, [None]),  \n",
    "        'days_posted' : tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "  \n",
    "    features = json_features_placeholder\n",
    "  \n",
    "    return tf.estimator.export.ServingInputReceiver(features, json_features_placeholder)\n",
    "\n",
    "## Create train and evaluate function using tf.estimator\n",
    "def train_and_evaluate(args):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(model_dir = args['output_dir'], save_summary_steps = 100, save_checkpoints_steps = 1000)\n",
    "  \n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        hidden_units =  args['hidden_units'],    #[1024, 512, 128, 32],  # specify neural architecture\n",
    "        feature_columns = featcols,\n",
    "        n_classes=2,\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "        #model_dir = OUTDIR,\n",
    "        config = run_config \n",
    "    )\n",
    "  \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = lambda : get_train_input_fn(args['train_data_paths']), max_steps = args['train_steps'])\n",
    "  \n",
    "    exporter_latest =  tf.estimator.LatestExporter('exporter', serving_input_receiver_fn = serving_input_fn)\n",
    "  \n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = lambda : get_valid_input_fn(args['eval_data_paths']), \n",
    "                                   steps = None,\n",
    "                                   start_delay_secs = args['eval_delay_secs'], # start evaluating after N seconds\n",
    "                                   throttle_secs = args['throttle_secs'],   # evaluate every N seconds\n",
    "                                   exporters = exporter_latest)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
