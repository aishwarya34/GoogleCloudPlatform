{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shread train-dev-test csv datasets progressively using BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max_farmhash  count\n",
      "0         None      0\n",
      "   min_farmhash10  max_farmhash10  count\n",
      "0             100             150  10333\n"
     ]
    }
   ],
   "source": [
    "def test_sample(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MAX(farmhash) as max_farmhash, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (\n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)  as farmhash, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2} )\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           a, b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "EVERY_N = 100\n",
    "query_maxhash = test_sample(0,70).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_maxhash = bq.Query(query_maxhash).execute().result().to_dataframe()\n",
    "print(df_maxhash)\n",
    "\n",
    "\n",
    "def test_sample2(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MIN(farmhash10) as min_farmhash10, MAX(farmhash10) as max_farmhash10, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (  \n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)*10  as farmhash10, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2})\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (10*10)+a, (10*10)+b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "  #return \"{}\\n{}\".format(basequery, sampler)\n",
    "\n",
    "\n",
    "EVERY_N = 100\n",
    "queryhash = test_sample2(0,60).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_hash = bq.Query(queryhash).execute().result().to_dataframe()\n",
    "print(df_hash.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_between(a, b, shredstart):\n",
    "  basequery = \"\"\"\n",
    "  SELECT \n",
    "    answer_count, comment_count, favorite_count,  score, view_count,\n",
    "    TIMESTAMP_DIFF(last_activity_date, creation_date, DAY) as days_posted,\n",
    "    IF(accepted_answer_id IS NULL , 0, 1 ) as accepted\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  \n",
    "  # Use sampling for initial model development. Once model is developed, shread the entire dataset into  .csv files based on condition in the sampler.\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < {1} AND MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= {0}\".format(\n",
    "            shredstart, shredstart + 10\n",
    "            )\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2}\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (shredstart*10)+a, (shredstart*10)+b\n",
    "          )\n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "\n",
    "def create_query(phase, EVERY_N, shredstart):\n",
    "  \"\"\"Phase: train (70%) valid (15%) or test (15%)\"\"\"\n",
    "  query = \"\"\n",
    "  if phase == 'train':\n",
    "    query = sample_between(0,60, shredstart)\n",
    "  elif phase == 'valid':\n",
    "    query = sample_between(60,75, shredstart)\n",
    "  else:\n",
    "    query = sample_between(75, 100, shredstart)\n",
    "  return query.replace(\"EVERY_N\", str(EVERY_N))\n",
    "\n",
    "#print(create_query('train', 100))\n",
    "#(answer_count - AVG(answer_count)) / STDDEV_POP(answer_count)  as answer_count,\n",
    "#IF(accepted_answer_id IS NULL , cast(0 as int64), cast(1 as int64)) as accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "  outdf = df.copy(deep = True)\n",
    "  #outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # Reorder columns so that target is first column\n",
    "  #print(outdf.head())\n",
    "  #print(df.head())\n",
    "  cols = outdf.columns.tolist()\n",
    "  #print(cols)\n",
    "  cols.remove('accepted')\n",
    "  cols.insert(0, 'accepted')\n",
    "  #print(cols)\n",
    "  outdf = outdf[cols]  \n",
    "  \n",
    "  \n",
    "  #Normalizing input columns  and replace NaN or null\n",
    "  normalize_cols = outdf.columns.tolist()\n",
    "  normalize_cols.remove('accepted')\n",
    "  for normalize_cols_name in normalize_cols:\n",
    "    outdf[normalize_cols_name].fillna(0, inplace = True)\n",
    "    outdf[normalize_cols_name] = (outdf[normalize_cols_name] - outdf[normalize_cols_name].mean())  / outdf[normalize_cols_name].std() \n",
    "  #print(outdf)\n",
    "  #print(outdf['answer_count'] )\n",
    "  outdf.to_csv(filename,  header = True, index_label = False, index = False)\n",
    "  print(\"Wrote {} to {}\".format(len(outdf), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10186 to stackoverflow-train-10.csv\n",
      "Wrote 10333 to stackoverflow-train-20.csv\n",
      "Wrote 3500 to stackoverflow-valid-10.csv\n",
      "Wrote 3453 to stackoverflow-valid-20.csv\n",
      "Wrote 3352 to stackoverflow-test-10.csv\n",
      "Wrote 3476 to stackoverflow-test-20.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "  #for x in range(10):\n",
    "  for x in range(2):\n",
    "    query = create_query(phase, 100, x*10)\n",
    "    #print(query)\n",
    "    df = bq.Query(query).execute().result().to_dataframe()\n",
    "    #print(df.head())\n",
    "    to_csv(df, 'stackoverflow-{}-{}.csv'.format(phase,(x+1)*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refactor by not using sampling and creating large shreaded datasets (check size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  412884 May 16 06:56 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  430803 May 16 06:56 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root 1272276 May 16 06:56 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1268321 May 16 06:56 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root  434375 May 16 06:56 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  430956 May 16 06:56 stackoverflow-valid-20.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "head: cannot open 'stackoverflow-test-100.csv' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "head stackoverflow-test-100.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 319kB/s eta 0:00:01  1% |▍                               | 1.2MB 29.5MB/s eta 0:00:04    6% |██▏                             | 6.3MB 35.7MB/s eta 0:00:03    15% |████▉                           | 14.0MB 34.2MB/s eta 0:00:03    20% |██████▌                         | 18.8MB 34.7MB/s eta 0:00:03    22% |███████                         | 20.4MB 28.7MB/s eta 0:00:03    27% |████████▋                       | 25.0MB 34.7MB/s eta 0:00:02    28% |█████████                       | 26.1MB 36.2MB/s eta 0:00:02    36% |███████████▋                    | 33.7MB 33.9MB/s eta 0:00:02    47% |███████████████▏                | 44.0MB 34.1MB/s eta 0:00:02    48% |███████████████▌                | 44.7MB 14.3MB/s eta 0:00:04    49% |███████████████▊                | 45.5MB 14.5MB/s eta 0:00:04    50% |████████████████▎               | 47.1MB 11.6MB/s eta 0:00:04    51% |████████████████▌               | 47.7MB 13.8MB/s eta 0:00:04    54% |█████████████████▋              | 50.8MB 35.9MB/s eta 0:00:02    56% |██████████████████              | 52.1MB 11.3MB/s eta 0:00:04    64% |████████████████████▋           | 59.5MB 33.3MB/s eta 0:00:01    67% |█████████████████████▌          | 62.2MB 31.8MB/s eta 0:00:01    76% |████████████████████████▍       | 70.4MB 28.4MB/s eta 0:00:01    84% |██████████████████████████▉     | 77.7MB 33.4MB/s eta 0:00:01    85% |███████████████████████████▍    | 79.3MB 35.9MB/s eta 0:00:01    88% |████████████████████████████▎   | 81.7MB 32.7MB/s eta 0:00:01    92% |█████████████████████████████▊  | 85.8MB 36.8MB/s eta 0:00:01    94% |██████████████████████████████▏ | 87.3MB 37.2MB/s eta 0:00:01    96% |██████████████████████████████▊ | 88.9MB 40.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 9.8MB/s eta 0:00:01    16% |█████▍                          | 532kB 35.3MB/s eta 0:00:01    63% |████████████████████▌           | 2.0MB 36.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.10.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.2.2)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 6.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 19.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.17.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (3.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.31.1)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.14.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.6.11)\n",
      "Requirement already satisfied: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.7.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (40.2.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/envs/py3env/lib/python3.5/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.2.0)\n",
      "Installing collected packages: tensorboard, keras-applications, tensorflow-estimator, keras-preprocessing, tensorflow\n",
      "  Found existing installation: tensorboard 1.8.0\n",
      "    Uninstalling tensorboard-1.8.0:\n",
      "      Successfully uninstalled tensorboard-1.8.0\n",
      "  Found existing installation: tensorflow 1.8.0\n",
      "    Uninstalling tensorflow-1.8.0:\n",
      "      Successfully uninstalled tensorflow-1.8.0\n",
      "Successfully installed keras-applications-1.0.7 keras-preprocessing-1.0.9 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
     ]
    }
   ],
   "source": [
    "# Ensure that we have TensorFlow 1.13.1 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  423423 May 16 06:13 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  414416 May 16 06:12 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  428712 May 16 06:12 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427717 May 16 06:12 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418516 May 16 06:12 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  440698 May 16 06:12 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  424388 May 16 06:12 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429867 May 16 06:12 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  441008 May 16 06:13 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427138 May 16 06:13 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1273856 May 16 06:11 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272276 May 16 06:09 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1269927 May 16 06:10 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1305190 May 16 06:10 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1279902 May 16 06:10 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1273932 May 16 06:10 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1295895 May 16 06:10 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1282789 May 16 06:10 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1270216 May 16 06:10 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1274448 May 16 06:10 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  418452 May 16 06:12 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  434471 May 16 06:11 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  428625 May 16 06:11 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  414987 May 16 06:11 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  442534 May 16 06:11 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  428649 May 16 06:11 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  427080 May 16 06:11 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  426023 May 16 06:11 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  432873 May 16 06:12 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431105 May 16 06:12 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Before reading csv was incorporated using pandas dataframe\n",
    "# But now reading csv is incorporated using tensorflow and so it's in the graps and also it reads progressively the shreaded files\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"./stackoverflow-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./stackoverflow-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./stackoverflow-test.csv\")\n",
    "\n",
    "CSV_COLUMNNAMES = list(df_train) # CSV_COLUMNNAMES = df_train.columns.tolist()\n",
    "print(CSV_COLUMNNAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMNNAMES[1:]\n",
    "LABEL_NAME = CSV_COLUMNNAMES[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3352 entries, 0 to 3351\n",
      "Data columns (total 7 columns):\n",
      "accepted          3352 non-null int64\n",
      "answer_count      3352 non-null float64\n",
      "comment_count     3352 non-null float64\n",
      "favorite_count    3352 non-null float64\n",
      "score             3352 non-null float64\n",
      "view_count        3352 non-null float64\n",
      "days_posted       3352 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 183.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Debugging  issue:  Field 0 in record 0 is not a valid int32: accepted\n",
    "#\t [[{{node DecodeCSV}}]]\n",
    "#\t [[node IteratorGetNext (defined at /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/util.py:110) ]]\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"stackoverflow-test-10.csv\")\n",
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "features: {'comment_count': <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, 'answer_count': <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, 'view_count': <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, 'days_posted': <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>, 'favorite_count': <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, 'score': <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>} \n",
      " label: Tensor(\"DecodeCSV:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({comment_count: (?,), answer_count: (?,), view_count: (?,), days_posted: (?,), favorite_count: (?,), score: (?,)}, (?,)), types: ({comment_count: tf.float32, answer_count: tf.float32, view_count: tf.float32, days_posted: tf.float32, favorite_count: tf.float32, score: tf.float32}, tf.int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_COLUMNS = ['accepted', 'answer_count', 'comment_count', 'favorite_count', 'score', 'view_count', 'days_posted']\n",
    "#DEFAULTS = [[], [], [], [], [], [], []]\n",
    "DEFAULTS = [[0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "#DEFAULTS = [tf.constant([0], dtype=tf.int32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#           tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32) ]\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def decode_line(row):\n",
    "    print(row)\n",
    "    cols = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    print(cols)\n",
    "    features = dict(zip(CSV_COLUMNS,cols))\n",
    "    print(cols)\n",
    "    label = features.pop('accepted')  # remove label from features and store\n",
    "    print(\"features: {} \\n label: {}\".format(features, label))\n",
    "    return features, label\n",
    "  \n",
    "  # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "  # Read lines from text files\n",
    "  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset).skip(1)\n",
    "  # Parse text lines as comma-separated values (CSV)\n",
    "  dataset = textlines_dataset.map(decode_line)\n",
    "  \n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "  \n",
    "  if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "    num_epochs = None  # loop indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size = 10*batch_size, seed=2)\n",
    "  else:\n",
    "    num_epochs = 1\n",
    "  \n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "  \n",
    "def get_train_input_fn():\n",
    "  return read_dataset('./stackoverflow-train-10.csv', tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "  return read_dataset('./stackoverflow-valid-10.csv', tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "def get_test_input_fn():\n",
    "  return read_dataset('./stackoverflow-test-10.csv', tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "get_train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = CSV_COLUMNS[1:]\n",
    "LABEL_NAME = CSV_COLUMNS[0]\n",
    "\n",
    "featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_protocol': None, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_tf_random_seed': 1, '_master': '', '_evaluation_master': '', '_service': None, '_save_checkpoints_steps': None, '_task_type': 'worker', '_model_dir': 'stackoverflow_model', '_save_summary_steps': 100, '_task_id': 0, '_keep_checkpoint_max': 5, '_experimental_distribute': None, '_log_step_count_steps': 100, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_is_chief': True, '_device_fn': None, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff2219579b0>, '_eval_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_secs': 600}\n",
      "Tensor(\"arg0:0\", shape=(), dtype=string, device=/device:CPU:0)\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "features: {'comment_count': <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, 'answer_count': <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, 'view_count': <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, 'days_posted': <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>, 'favorite_count': <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, 'score': <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>} \n",
      " label: Tensor(\"DecodeCSV:0\", shape=(), dtype=int32, device=/device:CPU:0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into stackoverflow_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 352.54797, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.9981\n",
      "INFO:tensorflow:loss = 260.78595, step = 101 (9.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into stackoverflow_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 236.05199.\n",
      "CPU times: user 20.4 s, sys: 448 ms, total: 20.8 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = \"stackoverflow_model\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)  \n",
    "  )\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda : get_train_input_fn(),\n",
    "    steps = 200\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string, device=/device:CPU:0)\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "features: {'comment_count': <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, 'answer_count': <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, 'view_count': <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, 'days_posted': <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>, 'favorite_count': <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, 'score': <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>} \n",
      " label: Tensor(\"DecodeCSV:0\", shape=(), dtype=int32, device=/device:CPU:0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16T07:11:40Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-07:11:42\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.64657146, accuracy_baseline = 0.524, auc = 0.71933013, auc_precision_recall = 0.69666255, average_loss = 0.5993068, global_step = 200, label/mean = 0.524, loss = 299.6534, precision = 0.6592, prediction/mean = 0.46159324, recall = 0.6739367\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: stackoverflow_model/model.ckpt-200\n",
      "RMSE on dataset = 0.7741490953447663\n"
     ]
    }
   ],
   "source": [
    "def validate_rmse(model):\n",
    "  metrices = model.evaluate(input_fn = lambda : get_valid_input_fn() )\n",
    "  print(\"RMSE on dataset = {}\".format(metrices[\"average_loss\"]**.5))\n",
    "\n",
    "#validate_rmse(model, df_train)\n",
    "validate_rmse(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on training set evaluate\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.6811679, accuracy_baseline = 0.5281519, auc = 0.7331483, auc_precision_recall = 0.7117538, average_loss = 0.56418276, global_step = 500, label/mean = 0.5281519, loss = 71.741234, precision = 0.6524123, prediction/mean = 0.48095724, recall = 0.8482496\n",
    "RMSE on dataset = 0.7511210011251841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string, device=/device:CPU:0)\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>]\n",
      "features: {'comment_count': <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, 'answer_count': <tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, 'view_count': <tf.Tensor 'DecodeCSV:5' shape=() dtype=float32>, 'days_posted': <tf.Tensor 'DecodeCSV:6' shape=() dtype=float32>, 'favorite_count': <tf.Tensor 'DecodeCSV:3' shape=() dtype=float32>, 'score': <tf.Tensor 'DecodeCSV:4' shape=() dtype=float32>} \n",
      " label: Tensor(\"DecodeCSV:0\", shape=(), dtype=int32, device=/device:CPU:0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model.predict( input_fn = lambda : get_test_input_fn() )\n",
    "\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "# class_ids determine the prediction\n",
    "\n",
    "predictions = [p['class_ids'][0] for p in raw_predictions]\n",
    "\n",
    "#confusion_matrix = tf.confusion_matrix(df_test['accepted'], predictions)\n",
    "#print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-48e40be9b234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accepted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test_predictions = df_test.copy(deep = True)\n",
    "df_test_predictions['accepted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(df_test, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_test_predictions, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
