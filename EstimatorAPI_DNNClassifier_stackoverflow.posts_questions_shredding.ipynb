{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shread train-dev-test csv datasets progressively using BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max_farmhash  count\n",
      "0         None      0\n",
      "   min_farmhash10  max_farmhash10  count\n",
      "0             100             150  10333\n"
     ]
    }
   ],
   "source": [
    "def test_sample(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MAX(farmhash) as max_farmhash, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (\n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)  as farmhash, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2} )\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           a, b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "EVERY_N = 100\n",
    "query_maxhash = test_sample(0,70).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_maxhash = bq.Query(query_maxhash).execute().result().to_dataframe()\n",
    "print(df_maxhash)\n",
    "\n",
    "\n",
    "def test_sample2(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MIN(farmhash10) as min_farmhash10, MAX(farmhash10) as max_farmhash10, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (  \n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)*10  as farmhash10, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2})\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (10*10)+a, (10*10)+b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "  #return \"{}\\n{}\".format(basequery, sampler)\n",
    "\n",
    "\n",
    "EVERY_N = 100\n",
    "queryhash = test_sample2(0,60).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_hash = bq.Query(queryhash).execute().result().to_dataframe()\n",
    "print(df_hash.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_between(a, b, shredstart):\n",
    "  basequery = \"\"\"\n",
    "  SELECT \n",
    "    answer_count, comment_count, favorite_count,  score, view_count,\n",
    "    TIMESTAMP_DIFF(last_activity_date, creation_date, DAY) as days_posted,\n",
    "    IF(accepted_answer_id IS NULL , 0, 1 ) as accepted\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  \n",
    "  # Use sampling for initial model development. Once model is developed, shread the entire dataset into  .csv files based on condition in the sampler.\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < {1} AND MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= {0}\".format(\n",
    "            shredstart, shredstart + 10\n",
    "            )\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2}\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (shredstart*10)+a, (shredstart*10)+b\n",
    "          )\n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "\n",
    "def create_query(phase, EVERY_N, shredstart):\n",
    "  \"\"\"Phase: train (70%) valid (15%) or test (15%)\"\"\"\n",
    "  query = \"\"\n",
    "  if phase == 'train':\n",
    "    query = sample_between(0,60, shredstart)\n",
    "  elif phase == 'valid':\n",
    "    query = sample_between(60,75, shredstart)\n",
    "  else:\n",
    "    query = sample_between(75, 100, shredstart)\n",
    "  return query.replace(\"EVERY_N\", str(EVERY_N))\n",
    "\n",
    "#print(create_query('train', 100))\n",
    "#(answer_count - AVG(answer_count)) / STDDEV_POP(answer_count)  as answer_count,\n",
    "#IF(accepted_answer_id IS NULL , cast(0 as int64), cast(1 as int64)) as accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "  outdf = df.copy(deep = True)\n",
    "  #outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # Reorder columns so that target is first column\n",
    "  #print(outdf.head())\n",
    "  #print(df.head())\n",
    "  cols = outdf.columns.tolist()\n",
    "  #print(cols)\n",
    "  cols.remove('accepted')\n",
    "  cols.insert(0, 'accepted')\n",
    "  #print(cols)\n",
    "  outdf = outdf[cols]  \n",
    "  \n",
    "  \n",
    "  #Normalizing input columns  and replace NaN or null\n",
    "  normalize_cols = outdf.columns.tolist()\n",
    "  normalize_cols.remove('accepted')\n",
    "  for normalize_cols_name in normalize_cols:\n",
    "    outdf[normalize_cols_name].fillna(0, inplace = True)\n",
    "    outdf[normalize_cols_name] = (outdf[normalize_cols_name] - outdf[normalize_cols_name].mean())  / outdf[normalize_cols_name].std() \n",
    "  #print(outdf)\n",
    "  #print(outdf['answer_count'] )\n",
    "  outdf.to_csv(filename,  header = False, index_label = False, index = False)\n",
    "  print(\"Wrote {} to {}\".format(len(outdf), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10186 to stackoverflow-train-10.csv\n",
      "Wrote 10333 to stackoverflow-train-20.csv\n",
      "Wrote 10426 to stackoverflow-train-30.csv\n",
      "Wrote 10260 to stackoverflow-train-40.csv\n",
      "Wrote 10298 to stackoverflow-train-50.csv\n",
      "Wrote 10401 to stackoverflow-train-60.csv\n",
      "Wrote 10276 to stackoverflow-train-70.csv\n",
      "Wrote 10249 to stackoverflow-train-80.csv\n",
      "Wrote 10291 to stackoverflow-train-90.csv\n",
      "Wrote 10332 to stackoverflow-train-100.csv\n",
      "Wrote 3500 to stackoverflow-valid-10.csv\n",
      "Wrote 3453 to stackoverflow-valid-20.csv\n",
      "Wrote 3367 to stackoverflow-valid-30.csv\n",
      "Wrote 3573 to stackoverflow-valid-40.csv\n",
      "Wrote 3482 to stackoverflow-valid-50.csv\n",
      "Wrote 3476 to stackoverflow-valid-60.csv\n",
      "Wrote 3431 to stackoverflow-valid-70.csv\n",
      "Wrote 3496 to stackoverflow-valid-80.csv\n",
      "Wrote 3469 to stackoverflow-valid-90.csv\n",
      "Wrote 3354 to stackoverflow-valid-100.csv\n",
      "Wrote 3352 to stackoverflow-test-10.csv\n",
      "Wrote 3476 to stackoverflow-test-20.csv\n",
      "Wrote 3439 to stackoverflow-test-30.csv\n",
      "Wrote 3408 to stackoverflow-test-40.csv\n",
      "Wrote 3571 to stackoverflow-test-50.csv\n",
      "Wrote 3454 to stackoverflow-test-60.csv\n",
      "Wrote 3441 to stackoverflow-test-70.csv\n",
      "Wrote 3594 to stackoverflow-test-80.csv\n",
      "Wrote 3441 to stackoverflow-test-90.csv\n",
      "Wrote 3405 to stackoverflow-test-100.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "  #for x in range(2):\n",
    "  for x in range(10):\n",
    "    query = create_query(phase, 100, x*10)\n",
    "    #print(query)\n",
    "    df = bq.Query(query).execute().result().to_dataframe()\n",
    "    #print(df.head())\n",
    "    to_csv(df, 'stackoverflow-{}-{}.csv'.format(phase,(x+1)*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refactor by not using sampling and creating large shreaded datasets (check size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  420995 May 16 08:59 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  414392 May 16 08:59 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  428369 May 16 08:59 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427660 May 16 08:59 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418436 May 16 08:59 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  440083 May 16 08:59 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  424308 May 16 08:59 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429799 May 16 08:59 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  444289 May 16 08:59 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427058 May 16 08:59 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1289503 May 16 08:58 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272389 May 16 08:57 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1276705 May 16 08:57 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1304776 May 16 08:57 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280210 May 16 08:57 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1278970 May 16 08:57 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1285149 May 16 08:57 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1283776 May 16 08:57 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1266554 May 16 08:58 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1277545 May 16 08:58 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  417483 May 16 08:59 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431702 May 16 08:58 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  430876 May 16 08:58 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413203 May 16 08:58 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  443636 May 16 08:58 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  427866 May 16 08:58 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  427021 May 16 08:58 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423116 May 16 08:58 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  432793 May 16 08:58 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431025 May 16 08:58 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,-0.3683210043696039,0.0008645057730129897,-0.16657359952844625,-0.08359529668588077,-0.16237617798053855,-0.25354894024689006\n",
      "1,-0.3683210043696039,0.0008645057730129897,-0.16657359952844625,-0.08359529668588077,-0.16115509227933522,-0.035211876510711476\n",
      "0,-0.3683210043696039,-0.735046033504261,-0.16657359952844625,-0.08359529668588077,-0.1956272809209988,-0.36481686695859644\n",
      "0,-0.3683210043696039,0.7367750450502869,-0.16657359952844625,-0.08359529668588077,-0.12997044206398833,-0.2955368371192321\n",
      "0,-0.3683210043696039,0.0008645057730129897,-0.16657359952844625,0.36269191751202456,-0.005043981863954096,2.1502581556946914\n",
      "1,-0.3683210043696039,-0.367090763865624,-0.16657359952844625,-0.08359529668588077,-0.19506370290505878,-0.36901565664583064\n",
      "1,-0.3683210043696039,-0.735046033504261,-0.16657359952844625,-0.17285273952546185,-0.18942792274565876,-0.35851868242774515\n",
      "1,0.3044971393420383,2.944506662882109,0.14370162278794293,3.8437321882556863,1.7170625755100517,-0.36901565664583064\n",
      "0,-0.3683210043696039,-0.735046033504261,-0.16657359952844625,-0.17285273952546185,-0.19496977323573547,-0.36901565664583064\n",
      "0,0.9773152830536805,-0.735046033504261,-0.16657359952844625,-0.17285273952546185,-0.18059853382926536,-0.2829404680575295\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "head stackoverflow-test-100.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==1.13.1\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure that we have TensorFlow 1.13.1 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  420995 May 16 08:59 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  414392 May 16 08:59 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  428369 May 16 08:59 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427660 May 16 08:59 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418436 May 16 08:59 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  440083 May 16 08:59 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  424308 May 16 08:59 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429799 May 16 08:59 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  444289 May 16 08:59 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427058 May 16 08:59 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1289503 May 16 08:58 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272389 May 16 08:57 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1276705 May 16 08:57 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1304776 May 16 08:57 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280210 May 16 08:57 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1278970 May 16 08:57 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1285149 May 16 08:57 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1283776 May 16 08:57 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1266554 May 16 08:58 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1277545 May 16 08:58 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  417483 May 16 08:59 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431702 May 16 08:58 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  430876 May 16 08:58 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413203 May 16 08:58 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  443636 May 16 08:58 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  427866 May 16 08:58 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  427021 May 16 08:58 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423116 May 16 08:58 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  432793 May 16 08:58 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431025 May 16 08:58 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Before reading csv was incorporated using pandas dataframe\n",
    "# But now reading csv is incorporated using tensorflow and so it's in the graps and also it reads progressively the shreaded files\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"./stackoverflow-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./stackoverflow-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./stackoverflow-test.csv\")\n",
    "\n",
    "CSV_COLUMNNAMES = list(df_train) # CSV_COLUMNNAMES = df_train.columns.tolist()\n",
    "print(CSV_COLUMNNAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMNNAMES[1:]\n",
    "LABEL_NAME = CSV_COLUMNNAMES[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10332 entries, 0 to 10331\n",
      "Data columns (total 7 columns):\n",
      "1                       10332 non-null int64\n",
      "6.453257689081337       10332 non-null float64\n",
      "-0.35101482510366283    10332 non-null float64\n",
      "6.353505428401191       10332 non-null float64\n",
      "16.54424381238194       10332 non-null float64\n",
      "3.201635366264088       10332 non-null float64\n",
      "5.228021343993179       10332 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 565.1 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3351 entries, 0 to 3350\n",
      "Data columns (total 7 columns):\n",
      "1                       3351 non-null int64\n",
      "0.12470310007233937     3351 non-null float64\n",
      "2.2582277091259537      3351 non-null float64\n",
      "-0.06564592296475838    3351 non-null float64\n",
      "-0.18404444736230954    3351 non-null float64\n",
      "-0.14508480486243064    3351 non-null float64\n",
      "-0.3741223082495156     3351 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 183.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Debugging  issue:  Field 0 in record 0 is not a valid int32: accepted\n",
    "#\t [[{{node DecodeCSV}}]]\n",
    "#\t [[node IteratorGetNext (defined at /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/util.py:110) ]]\n",
    "\n",
    "# Solution :  Skip the first line (header row) \n",
    "# skip(count)  :  Creates a Dataset that skips count elements from this dataset.\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"stackoverflow-train-20.csv\")\n",
    "df_train.info()\n",
    "\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"stackoverflow-test-10.csv\")\n",
    "df_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['accepted', 'answer_count', 'comment_count', 'favorite_count', 'score', 'view_count', 'days_posted']\n",
    "DEFAULTS = [[0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "#DEFAULTS = [tf.constant([0], dtype=tf.int32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#           tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32) ]\n",
    "\n",
    "#i=0\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def decode_line(row):\n",
    "    #print(row)\n",
    "    cols = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    #print(cols)\n",
    "    features = dict(zip(CSV_COLUMNS,cols))\n",
    "    #print(i+1)\n",
    "    label = features.pop('accepted')  # remove label from features and store\n",
    "    #print(\"features: {} \\n label: {}\".format(features, label))\n",
    "    return features, label\n",
    "  \n",
    "  # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "  # Read lines from text files\n",
    "  #textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset).skip(1)\n",
    "  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "  # Parse text lines as comma-separated values (CSV)\n",
    "  dataset = textlines_dataset.map(decode_line)\n",
    "  \n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "  \n",
    "  if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "    num_epochs = 10  # loop indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size = 10*batch_size, seed=2)\n",
    "  else:\n",
    "    num_epochs = 1\n",
    "  \n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "  \n",
    "def get_train_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-train-*.csv', tf.estimator.ModeKeys.TRAIN)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  #print(\"Training set :  \\nfeatures1 : {}\\nlabel: {}\".format(features1, label1))\n",
    "  with tf.Session() as sess:\n",
    "    print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "  return features1, label1 \n",
    "\n",
    "def get_valid_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-valid-*.csv', tf.estimator.ModeKeys.EVAL)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  return features1, label1 \n",
    "\n",
    "def get_test_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-test-*.csv', tf.estimator.ModeKeys.PREDICT)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  with tf.Session() as sess:\n",
    "    print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "  return features1, label1 \n",
    "\n",
    "\n",
    "#get_train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = CSV_COLUMNS[1:]\n",
    "LABEL_NAME = CSV_COLUMNS[0]\n",
    "\n",
    "featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_train_distribute': None, '_save_summary_steps': 100, '_tf_random_seed': 1, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd446cb57f0>, '_task_id': 0, '_master': '', '_model_dir': 'stackoverflow_model', '_protocol': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_service': None, '_eval_distribute': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_device_fn': None}\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Training set :  \n",
      "features1 : {'answer_count': <tf.Tensor 'IteratorGetNext:0' shape=(?,) dtype=float32>, 'score': <tf.Tensor 'IteratorGetNext:4' shape=(?,) dtype=float32>, 'comment_count': <tf.Tensor 'IteratorGetNext:1' shape=(?,) dtype=float32>, 'view_count': <tf.Tensor 'IteratorGetNext:5' shape=(?,) dtype=float32>, 'favorite_count': <tf.Tensor 'IteratorGetNext:3' shape=(?,) dtype=float32>, 'days_posted': <tf.Tensor 'IteratorGetNext:2' shape=(?,) dtype=float32>}\n",
      "label: Tensor(\"IteratorGetNext:6\", shape=(?,), dtype=int32, device=/device:CPU:0)\n",
      "[512]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into stackoverflow_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 354.78867, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.7633\n",
      "INFO:tensorflow:loss = 294.24915, step = 101 (9.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5553\n",
      "INFO:tensorflow:loss = 325.40585, step = 201 (9.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1091\n",
      "INFO:tensorflow:loss = 287.79752, step = 301 (9.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0452\n",
      "INFO:tensorflow:loss = 307.41013, step = 401 (9.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1474\n",
      "INFO:tensorflow:loss = 256.3389, step = 501 (8.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0919\n",
      "INFO:tensorflow:loss = 308.3885, step = 601 (9.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1139\n",
      "INFO:tensorflow:loss = 263.5327, step = 701 (8.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1579\n",
      "INFO:tensorflow:loss = 311.79672, step = 801 (8.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2411\n",
      "INFO:tensorflow:loss = 260.9312, step = 901 (8.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0137\n",
      "INFO:tensorflow:loss = 311.69342, step = 1001 (9.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1378\n",
      "INFO:tensorflow:loss = 226.45224, step = 1101 (8.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1865\n",
      "INFO:tensorflow:loss = 297.99625, step = 1201 (8.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1581\n",
      "INFO:tensorflow:loss = 245.8868, step = 1301 (8.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1634\n",
      "INFO:tensorflow:loss = 304.65906, step = 1401 (8.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1208\n",
      "INFO:tensorflow:loss = 291.7557, step = 1501 (8.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2263\n",
      "INFO:tensorflow:loss = 300.21002, step = 1601 (8.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1885\n",
      "INFO:tensorflow:loss = 310.86084, step = 1701 (8.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0856\n",
      "INFO:tensorflow:loss = 299.49374, step = 1801 (9.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1042\n",
      "INFO:tensorflow:loss = 321.16705, step = 1901 (9.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3397\n",
      "INFO:tensorflow:loss = 308.1923, step = 2001 (8.818 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2013 into stackoverflow_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 225.47038.\n",
      "CPU times: user 2min 58s, sys: 3.73 s, total: 3min 2s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = \"stackoverflow_model\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)  \n",
    "  )\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda : get_train_input_fn()\n",
    "    #,steps = 200\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16T09:04:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-2013\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-09:04:37\n",
      "INFO:tensorflow:Saving dict for global step 2013: accuracy = 0.6735644, accuracy_baseline = 0.5301003, auc = 0.732806, auc_precision_recall = 0.7207757, average_loss = 0.55780685, global_step = 2013, label/mean = 0.5301003, loss = 283.83344, precision = 0.6406194, prediction/mean = 0.52562964, recall = 0.8751499\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2013: stackoverflow_model/model.ckpt-2013\n",
      "RMSE on dataset = 0.7468646794966778\n"
     ]
    }
   ],
   "source": [
    "def validate_rmse(model):\n",
    "  metrices = model.evaluate(input_fn = lambda : get_valid_input_fn() )\n",
    "  print(\"RMSE on dataset = {}\".format(metrices[\"average_loss\"]**.5))\n",
    "\n",
    "#validate_rmse(model, df_train)\n",
    "validate_rmse(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "validate_rmse for 10 epochs on 2 validate csvs:\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 401: accuracy = 0.67553574, accuracy_baseline = 0.5273982, auc = 0.73186123, auc_precision_recall = 0.7124213, average_loss = 0.5643283, global_step = 401, label/mean = 0.5273982, loss = 280.26962, precision = 0.6314025, prediction/mean = 0.53914005, recall = 0.9244614\n",
    "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 401: stackoverflow_model/model.ckpt-401\n",
    "RMSE on dataset = 0.751217886417676\n",
    "\n",
    "\n",
    "\n",
    "on training set evaluate\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.6811679, accuracy_baseline = 0.5281519, auc = 0.7331483, auc_precision_recall = 0.7117538, average_loss = 0.56418276, global_step = 500, label/mean = 0.5281519, loss = 71.741234, precision = 0.6524123, prediction/mean = 0.48095724, recall = 0.8482496\n",
    "RMSE on dataset = 0.7511210011251841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-2013\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[34581]\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model.predict( input_fn = lambda : get_test_input_fn() )\n",
    "\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "# class_ids determine the prediction\n",
    "\n",
    "predictions = [p['class_ids'][0] for p in raw_predictions]\n",
    "#confusion_matrix = tf.confusion_matrix(df_test['accepted'], predictions)\n",
    "#print(confusion_matrix)\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run(tf.shape(predictions))) # output: [ 0.42116176  0.40666069]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_one_shot_iterator()  \n",
    "\n",
    "##### Parse all the *.csv files using make_one_shot_iterator()  \n",
    "\n",
    "##### How it works:\n",
    "\n",
    "Suppose we have only 2.csv files having #entries 10185 and 10332\n",
    "\n",
    "Total number of iterations when make_one_shot_iterator() is used:   401\n",
    "\n",
    "\n",
    "= ( 10185*10 + 10332*10 ) / 512\n",
    "\n",
    "= ( 101850 + 103320 ) / 512 =  400.72265625\n",
    "\n",
    "where \n",
    "  - 10 = 10 epochs\n",
    "  - 512 = batch size\n",
    "  - 10185*10  = 1st csv total no of entries to be iterated over\n",
    "  - 10332*10  = 2st csv total no of entries to be iterated over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-48e40be9b234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accepted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test_predictions = df_test.copy(deep = True)\n",
    "df_test_predictions['accepted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(df_test, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_test_predictions, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
