{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shread train-dev-test csv datasets progressively using BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max_farmhash  count\n",
      "0         None      0\n",
      "   min_farmhash10  max_farmhash10  count\n",
      "0             100             150  10333\n"
     ]
    }
   ],
   "source": [
    "def test_sample(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MAX(farmhash) as max_farmhash, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (\n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)  as farmhash, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2} )\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           a, b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "EVERY_N = 100\n",
    "query_maxhash = test_sample(0,70).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_maxhash = bq.Query(query_maxhash).execute().result().to_dataframe()\n",
    "print(df_maxhash)\n",
    "\n",
    "\n",
    "def test_sample2(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MIN(farmhash10) as min_farmhash10, MAX(farmhash10) as max_farmhash10, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (  \n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)*10  as farmhash10, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2})\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (10*10)+a, (10*10)+b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "  #return \"{}\\n{}\".format(basequery, sampler)\n",
    "\n",
    "\n",
    "EVERY_N = 100\n",
    "queryhash = test_sample2(0,60).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_hash = bq.Query(queryhash).execute().result().to_dataframe()\n",
    "print(df_hash.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_between(a, b, shredstart):\n",
    "  basequery = \"\"\"\n",
    "  SELECT \n",
    "    answer_count, comment_count, favorite_count,  score, view_count,\n",
    "    TIMESTAMP_DIFF(last_activity_date, creation_date, DAY) as days_posted,\n",
    "    IF(accepted_answer_id IS NULL , 0, 1 ) as accepted\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  \n",
    "  # Use sampling for initial model development. Once model is developed, shread the entire dataset into  .csv files based on condition in the sampler.\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < {1} AND MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= {0}\".format(\n",
    "            shredstart, shredstart + 10\n",
    "            )\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2}\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (shredstart*10)+a, (shredstart*10)+b\n",
    "          )\n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "\n",
    "def create_query(phase, EVERY_N, shredstart):\n",
    "  \"\"\"Phase: train (70%) valid (15%) or test (15%)\"\"\"\n",
    "  query = \"\"\n",
    "  if phase == 'train':\n",
    "    query = sample_between(0,60, shredstart)\n",
    "  elif phase == 'valid':\n",
    "    query = sample_between(60,75, shredstart)\n",
    "  else:\n",
    "    query = sample_between(75, 100, shredstart)\n",
    "  return query.replace(\"EVERY_N\", str(EVERY_N))\n",
    "\n",
    "#print(create_query('train', 100))\n",
    "#(answer_count - AVG(answer_count)) / STDDEV_POP(answer_count)  as answer_count,\n",
    "#IF(accepted_answer_id IS NULL , cast(0 as int64), cast(1 as int64)) as accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "  outdf = df.copy(deep = True)\n",
    "  #outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # Reorder columns so that target is first column\n",
    "  #print(outdf.head())\n",
    "  #print(df.head())\n",
    "  cols = outdf.columns.tolist()\n",
    "  #print(cols)\n",
    "  cols.remove('accepted')\n",
    "  cols.insert(0, 'accepted')\n",
    "  #print(cols)\n",
    "  outdf = outdf[cols]  \n",
    "  \n",
    "  \n",
    "  #Normalizing input columns  and replace NaN or null\n",
    "  normalize_cols = outdf.columns.tolist()\n",
    "  normalize_cols.remove('accepted')\n",
    "  for normalize_cols_name in normalize_cols:\n",
    "    outdf[normalize_cols_name].fillna(0, inplace = True)\n",
    "    outdf[normalize_cols_name] = (outdf[normalize_cols_name] - outdf[normalize_cols_name].mean())  / outdf[normalize_cols_name].std() \n",
    "  #print(outdf)\n",
    "  #print(outdf['answer_count'] )\n",
    "  outdf.to_csv(filename,  header = False, index_label = False, index = False)\n",
    "  print(\"Wrote {} to {}\".format(len(outdf), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10186 to stackoverflow-train-10.csv\n",
      "Wrote 10333 to stackoverflow-train-20.csv\n",
      "Wrote 10426 to stackoverflow-train-30.csv\n",
      "Wrote 10260 to stackoverflow-train-40.csv\n",
      "Wrote 10298 to stackoverflow-train-50.csv\n",
      "Wrote 10401 to stackoverflow-train-60.csv\n",
      "Wrote 10276 to stackoverflow-train-70.csv\n",
      "Wrote 10249 to stackoverflow-train-80.csv\n",
      "Wrote 10291 to stackoverflow-train-90.csv\n",
      "Wrote 10332 to stackoverflow-train-100.csv\n",
      "Wrote 3500 to stackoverflow-valid-10.csv\n",
      "Wrote 3453 to stackoverflow-valid-20.csv\n",
      "Wrote 3367 to stackoverflow-valid-30.csv\n",
      "Wrote 3573 to stackoverflow-valid-40.csv\n",
      "Wrote 3482 to stackoverflow-valid-50.csv\n",
      "Wrote 3476 to stackoverflow-valid-60.csv\n",
      "Wrote 3431 to stackoverflow-valid-70.csv\n",
      "Wrote 3496 to stackoverflow-valid-80.csv\n",
      "Wrote 3469 to stackoverflow-valid-90.csv\n",
      "Wrote 3354 to stackoverflow-valid-100.csv\n",
      "Wrote 3352 to stackoverflow-test-10.csv\n",
      "Wrote 3476 to stackoverflow-test-20.csv\n",
      "Wrote 3439 to stackoverflow-test-30.csv\n",
      "Wrote 3408 to stackoverflow-test-40.csv\n",
      "Wrote 3571 to stackoverflow-test-50.csv\n",
      "Wrote 3454 to stackoverflow-test-60.csv\n",
      "Wrote 3441 to stackoverflow-test-70.csv\n",
      "Wrote 3594 to stackoverflow-test-80.csv\n",
      "Wrote 3441 to stackoverflow-test-90.csv\n",
      "Wrote 3405 to stackoverflow-test-100.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "  #for x in range(2):\n",
    "  for x in range(10):\n",
    "    query = create_query(phase, 100, x*10)\n",
    "    #print(query)\n",
    "    df = bq.Query(query).execute().result().to_dataframe()\n",
    "    #print(df.head())\n",
    "    to_csv(df, 'stackoverflow-{}-{}.csv'.format(phase,(x+1)*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refactor by not using sampling and creating large shreaded datasets (check size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  420953 May 16 19:56 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  410883 May 16 19:55 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  430781 May 16 19:55 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427660 May 16 19:55 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418436 May 16 19:55 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  441018 May 16 19:55 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  425796 May 16 19:55 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429537 May 16 19:55 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  443612 May 16 19:55 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427058 May 16 19:56 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1276647 May 16 19:54 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272196 May 16 19:52 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1268241 May 16 19:52 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1304954 May 16 19:52 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280210 May 16 19:53 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1278970 May 16 19:53 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1286367 May 16 19:53 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1283776 May 16 19:53 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1266617 May 16 19:53 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1283209 May 16 19:53 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  418194 May 16 19:55 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431702 May 16 19:54 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  428810 May 16 19:54 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413285 May 16 19:54 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  442332 May 16 19:54 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  427866 May 16 19:54 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  425989 May 16 19:54 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423116 May 16 19:54 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  434830 May 16 19:54 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431025 May 16 19:54 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,-1.0411391480812462,-0.367090763865624,-0.16657359952844625,-0.17285273952546185,-0.19863303033934546,-0.36901565664583064\n",
      "1,0.3044971393420383,0.0008645057730129897,-0.16657359952844625,-0.08359529668588077,0.0982786543917133,-0.36901565664583064\n",
      "1,-0.3683210043696039,0.7367750450502869,-0.16657359952844625,0.005662146153700297,-0.1545800154267018,-0.07929916822667062\n",
      "1,-0.3683210043696039,-0.367090763865624,-0.16657359952844625,-0.351367625204624,0.03017964413229615,-0.36901565664583064\n",
      "1,-0.3683210043696039,0.0008645057730129897,0.4539768451043321,-0.08359529668588077,0.19286583140031063,0.33218222112228135\n",
      "1,0.3044971393420383,0.0008645057730129897,0.4539768451043321,-0.08359529668588077,0.07056940194132977,-0.35431989274051096\n",
      "0,-0.3683210043696039,-0.367090763865624,0.14370162278794293,-0.17285273952546185,-0.19534549191302877,-0.36901565664583064\n",
      "0,-1.0411391480812462,0.36881977541164995,-0.16657359952844625,-0.08359529668588077,-0.19750587430746544,-0.36901565664583064\n",
      "1,-0.3683210043696039,-0.735046033504261,-0.16657359952844625,-0.17285273952546185,-0.16660301310008857,-0.35012110305327676\n",
      "0,-0.3683210043696039,0.7367750450502869,-0.16657359952844625,-0.17285273952546185,-0.1868918216739287,-0.36901565664583064\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "head stackoverflow-test-100.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 186kB/s eta 0:00:01  0% |▎                               | 716kB 36.7MB/s eta 0:00:03    10% |███▏                            | 9.3MB 25.9MB/s eta 0:00:04    13% |████▎                           | 12.3MB 10.0MB/s eta 0:00:08    16% |█████▏                          | 15.1MB 34.0MB/s eta 0:00:03    17% |█████▊                          | 16.6MB 34.1MB/s eta 0:00:03    20% |██████▊                         | 19.3MB 28.1MB/s eta 0:00:03    24% |███████▊                        | 22.2MB 34.5MB/s eta 0:00:03    26% |████████▋                       | 24.8MB 8.9MB/s eta 0:00:08    29% |█████████▋                      | 27.8MB 15.4MB/s eta 0:00:05    37% |████████████                    | 34.7MB 14.8MB/s eta 0:00:04    38% |████████████▎                   | 35.5MB 7.4MB/s eta 0:00:08    39% |████████████▌                   | 36.3MB 31.2MB/s eta 0:00:02    43% |██████████████                  | 40.3MB 28.9MB/s eta 0:00:02    47% |███████████████▏                | 43.9MB 9.5MB/s eta 0:00:06    50% |████████████████                | 46.4MB 34.2MB/s eta 0:00:02    51% |████████████████▌               | 47.7MB 30.3MB/s eta 0:00:02    53% |█████████████████               | 49.1MB 20.4MB/s eta 0:00:03    54% |█████████████████▌              | 50.6MB 31.4MB/s eta 0:00:02    56% |██████████████████              | 51.9MB 10.1MB/s eta 0:00:05    57% |██████████████████▌             | 53.5MB 40.1MB/s eta 0:00:01    59% |███████████████████             | 55.1MB 34.8MB/s eta 0:00:02    61% |███████████████████▋            | 56.7MB 33.9MB/s eta 0:00:02    64% |████████████████████▊           | 59.8MB 34.4MB/s eta 0:00:01    66% |█████████████████████▏          | 61.3MB 34.5MB/s eta 0:00:01    69% |██████████████████████          | 63.9MB 34.1MB/s eta 0:00:01    70% |██████████████████████▌         | 64.9MB 11.4MB/s eta 0:00:03    71% |███████████████████████         | 66.4MB 27.9MB/s eta 0:00:01    74% |████████████████████████        | 69.1MB 7.9MB/s eta 0:00:03    75% |████████████████████████        | 69.6MB 28.4MB/s eta 0:00:01    87% |████████████████████████████    | 80.9MB 32.4MB/s eta 0:00:01    89% |████████████████████████████▌   | 82.5MB 31.3MB/s eta 0:00:01    99% |███████████████████████████████▉| 92.2MB 39.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 21.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.6.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.2.2)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 9.0MB/s eta 0:00:01  2% |▉                               | 81kB 9.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.14.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.31.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.17.1)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 5.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.10.0)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 11.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mock>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.6.11)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (40.2.0)\n",
      "Requirement already satisfied: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.7.1)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/envs/py3env/lib/python3.5/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.2.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, tensorflow\n",
      "  Found existing installation: tensorboard 1.8.0\n",
      "    Uninstalling tensorboard-1.8.0:\n",
      "      Successfully uninstalled tensorboard-1.8.0\n",
      "  Found existing installation: tensorflow 1.8.0\n",
      "    Uninstalling tensorflow-1.8.0:\n",
      "      Successfully uninstalled tensorflow-1.8.0\n",
      "Successfully installed keras-applications-1.0.7 keras-preprocessing-1.0.9 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
     ]
    }
   ],
   "source": [
    "# Ensure that we have TensorFlow 1.13.1 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from google.datalab.ml import TensorBoard\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  420953 May 16 19:56 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  410883 May 16 19:55 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  430781 May 16 19:55 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427660 May 16 19:55 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418436 May 16 19:55 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  441018 May 16 19:55 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  425796 May 16 19:55 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429537 May 16 19:55 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  443612 May 16 19:55 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427058 May 16 19:56 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1276647 May 16 19:54 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272196 May 16 19:52 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1268241 May 16 19:52 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1304954 May 16 19:52 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280210 May 16 19:53 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1278970 May 16 19:53 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1286367 May 16 19:53 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1283776 May 16 19:53 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1266617 May 16 19:53 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1283209 May 16 19:53 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  418194 May 16 19:55 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431702 May 16 19:54 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  428810 May 16 19:54 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413285 May 16 19:54 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  442332 May 16 19:54 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  427866 May 16 19:54 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  425989 May 16 19:54 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423116 May 16 19:54 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  434830 May 16 19:54 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431025 May 16 19:54 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Before reading csv was incorporated using pandas dataframe\n",
    "# But now reading csv is incorporated using tensorflow and so it's in the graps and also it reads progressively the shreaded files\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"./stackoverflow-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./stackoverflow-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./stackoverflow-test.csv\")\n",
    "\n",
    "CSV_COLUMNNAMES = list(df_train) # CSV_COLUMNNAMES = df_train.columns.tolist()\n",
    "print(CSV_COLUMNNAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMNNAMES[1:]\n",
    "LABEL_NAME = CSV_COLUMNNAMES[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10332 entries, 0 to 10331\n",
      "Data columns (total 7 columns):\n",
      "0                       10332 non-null int64\n",
      "9.17859509730161        10332 non-null float64\n",
      "0.016391595864342398    10332 non-null float64\n",
      "6.613646539294375       10332 non-null float64\n",
      "10.451072671114654      10332 non-null float64\n",
      "4.223676425737634       10332 non-null float64\n",
      "6.5386884140305765      10332 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 565.1 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3351 entries, 0 to 3350\n",
      "Data columns (total 7 columns):\n",
      "1                       3351 non-null int64\n",
      "0.12470310007233937     3351 non-null float64\n",
      "2.2582277091259537      3351 non-null float64\n",
      "-0.0656459229647584     3351 non-null float64\n",
      "-0.18404444736230954    3351 non-null float64\n",
      "-0.14508480486243067    3351 non-null float64\n",
      "-0.3741223082495156     3351 non-null float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 183.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Debugging  issue:  Field 0 in record 0 is not a valid int32: accepted\n",
    "#\t [[{{node DecodeCSV}}]]\n",
    "#\t [[node IteratorGetNext (defined at /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/util.py:110) ]]\n",
    "\n",
    "# Solution :  Skip the first line (header row) \n",
    "# skip(count)  :  Creates a Dataset that skips count elements from this dataset.\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"stackoverflow-train-20.csv\")\n",
    "df_train.info()\n",
    "\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"stackoverflow-test-10.csv\")\n",
    "df_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['accepted', 'answer_count', 'comment_count', 'favorite_count', 'score', 'view_count', 'days_posted']\n",
    "DEFAULTS = [[0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "#DEFAULTS = [tf.constant([0], dtype=tf.int32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#           tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32),\n",
    "#            tf.constant([0.0], dtype=tf.float32) ]\n",
    "\n",
    "#i=0\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def decode_line(row):\n",
    "    #print(row)\n",
    "    cols = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    #print(cols)\n",
    "    features = dict(zip(CSV_COLUMNS,cols))\n",
    "    #print(i+1)\n",
    "    label = features.pop('accepted')  # remove label from features and store\n",
    "    #print(\"features: {} \\n label: {}\".format(features, label))\n",
    "    return features, label\n",
    "  \n",
    "  # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "  # Read lines from text files\n",
    "  #textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset).skip(1)\n",
    "  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "  # Parse text lines as comma-separated values (CSV)\n",
    "  dataset = textlines_dataset.map(decode_line)\n",
    "  \n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "  \n",
    "  if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "    num_epochs = 10  # loop indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size = 10*batch_size, seed=2)\n",
    "  else:\n",
    "    num_epochs = 1\n",
    "  \n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "  \n",
    "def get_train_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-train-*.csv', tf.estimator.ModeKeys.TRAIN)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  #print(\"Training set :  \\nfeatures1 : {}\\nlabel: {}\".format(features1, label1))\n",
    "  with tf.Session() as sess:\n",
    "    print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "  return features1, label1 \n",
    "\n",
    "def get_valid_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-valid-*.csv', tf.estimator.ModeKeys.EVAL)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  return features1, label1 \n",
    "\n",
    "def get_test_input_fn():\n",
    "  dataset = read_dataset('./stackoverflow-test-*.csv', tf.estimator.ModeKeys.PREDICT)\n",
    "  features1, label1 = dataset.make_one_shot_iterator().get_next()\n",
    "  with tf.Session() as sess:\n",
    "    print(sess.run(tf.shape(label1))) # output: [ 0.42116176  0.40666069]\n",
    "  return features1, label1 \n",
    "\n",
    "\n",
    "#get_train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = CSV_COLUMNS[1:]\n",
    "LABEL_NAME = CSV_COLUMNS[0]\n",
    "\n",
    "featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refactored for Distributed training and Monitoring \n",
    "'''\n",
    "%%time\n",
    "OUTDIR = \"stackoverflow_model\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)  \n",
    "  )\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda : get_train_input_fn()\n",
    "    #,steps = 200\n",
    "  )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refactored for Distributed training and Monitoring \n",
    "\n",
    "'''\n",
    "def validate_rmse(model):\n",
    "  metrices = model.evaluate(input_fn = lambda : get_valid_input_fn() )\n",
    "  print(\"RMSE on dataset = {}\".format(metrices[\"average_loss\"]**.5))\n",
    "\n",
    "#validate_rmse(model, df_train)\n",
    "validate_rmse(model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving input function\n",
    "Defines the expected shape of the JSON feed that the model will receive once deployed behind a REST API in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "  \n",
    "  json_features_placeholder = {\n",
    "    'answer_count' : tf.placeholder(tf.float32, [None]), #Batch size\n",
    "    'comment_count' : tf.placeholder(tf.float32, [None]), \n",
    "    'favorite_count' : tf.placeholder(tf.float32, [None]), \n",
    "    'score' : tf.placeholder(tf.float32, [None]), \n",
    "    'view_count' : tf.placeholder(tf.float32, [None]),  \n",
    "    'days_posted' : tf.placeholder(tf.float32, [None])\n",
    "  }\n",
    "  \n",
    "  features = json_features_placeholder\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(features, json_features_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.estimator.train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train and evaluate function using tf.estimator\n",
    "def train_and_evaluate(outputdir, num_train_steps):\n",
    "\n",
    "  run_config = tf.estimator.RunConfig(model_dir = outputdir, save_summary_steps = 100, save_checkpoints_steps = 1000)\n",
    "  \n",
    "  estimator = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    #model_dir = OUTDIR,\n",
    "    config = run_config \n",
    "  )\n",
    "  \n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = lambda : get_train_input_fn(), max_steps = num_train_steps)\n",
    "  \n",
    "  exporter_latest =  tf.estimator.LatestExporter('exporter', serving_input_receiver_fn = serving_input_fn)\n",
    "  \n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = lambda : get_valid_input_fn(), \n",
    "                                   steps = None,\n",
    "                                   start_delay_secs = 1, # start evaluating after N seconds\n",
    "                                   throttle_secs = 10,   # evaluate every N seconds\n",
    "                                   exporters = exporter_latest)\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring with TensorBoard\n",
    "Use \"refresh\" in Tensorboard during training to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 8021. Click <a href=\"/_proxy/58491/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8021"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTDIR = \"stackoverflow_model\"\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_model_dir': 'stackoverflow_model', '_protocol': None, '_save_summary_steps': 100, '_eval_distribute': None, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_global_id_in_cluster': 0, '_save_checkpoints_steps': 1000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f22e869ef98>, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_num_worker_replicas': 1, '_master': '', '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_experimental_distribute': None}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "[512]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into stackoverflow_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 357.5438, step = 1\n",
      "INFO:tensorflow:global_step/sec: 10.0291\n",
      "INFO:tensorflow:loss = 289.7368, step = 101 (9.979 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1337\n",
      "INFO:tensorflow:loss = 291.38733, step = 201 (9.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0129\n",
      "INFO:tensorflow:loss = 288.52985, step = 301 (9.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1822\n",
      "INFO:tensorflow:loss = 307.9926, step = 401 (9.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1776\n",
      "INFO:tensorflow:loss = 286.01935, step = 501 (9.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2162\n",
      "INFO:tensorflow:loss = 309.4718, step = 601 (9.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2579\n",
      "INFO:tensorflow:loss = 256.5998, step = 701 (9.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1924\n",
      "INFO:tensorflow:loss = 303.6145, step = 801 (9.812 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.117\n",
      "INFO:tensorflow:loss = 265.2041, step = 901 (9.884 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into stackoverflow_model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16T20:40:02Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-20:40:06\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.6731308, accuracy_baseline = 0.5301003, auc = 0.73026127, auc_precision_recall = 0.7179193, average_loss = 0.55979574, global_step = 1000, label/mean = 0.5301003, loss = 284.84546, precision = 0.63415754, prediction/mean = 0.5274116, recall = 0.9061171\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: stackoverflow_model/model.ckpt-1000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'score': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'comment_count': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'view_count': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'favorite_count': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'answer_count': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'days_posted': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'score': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'comment_count': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'view_count': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'favorite_count': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'answer_count': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'days_posted': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'score': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'comment_count': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'view_count': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'favorite_count': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'answer_count': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'days_posted': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: stackoverflow_model/export/exporter/temp-b'1558039206'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 308.70886.\n",
      "CPU times: user 1min 39s, sys: 2.16 s, total: 1min 41s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "train_and_evaluate(OUTDIR, num_train_steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "validate_rmse for 10 epochs on 2 validate csvs:\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 401: accuracy = 0.67553574, accuracy_baseline = 0.5273982, auc = 0.73186123, auc_precision_recall = 0.7124213, average_loss = 0.5643283, global_step = 401, label/mean = 0.5273982, loss = 280.26962, precision = 0.6314025, prediction/mean = 0.53914005, recall = 0.9244614\n",
    "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 401: stackoverflow_model/model.ckpt-401\n",
    "RMSE on dataset = 0.751217886417676\n",
    "\n",
    "\n",
    "\n",
    "on training set evaluate\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.6811679, accuracy_baseline = 0.5281519, auc = 0.7331483, auc_precision_recall = 0.7117538, average_loss = 0.56418276, global_step = 500, label/mean = 0.5281519, loss = 71.741234, precision = 0.6524123, prediction/mean = 0.48095724, recall = 0.8482496\n",
    "RMSE on dataset = 0.7511210011251841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can now shut Tensorboard down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logdir</th>\n",
       "      <th>pid</th>\n",
       "      <th>port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>taxi_trained</td>\n",
       "      <td>3862</td>\n",
       "      <td>40839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taxi_trained</td>\n",
       "      <td>3956</td>\n",
       "      <td>33539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stackoverflow_model</td>\n",
       "      <td>8021</td>\n",
       "      <td>58491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                logdir   pid   port\n",
       "0         taxi_trained  3862  40839\n",
       "1         taxi_trained  3956  33539\n",
       "2  stackoverflow_model  8021  58491"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to list Tensorboard instances\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop TensorBoard fill the correct pid below\n",
    "TensorBoard().stop(33539)\n",
    "print(\"Stopped Tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512]\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from stackoverflow_model/model.ckpt-2013\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[34581]\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model.predict( input_fn = lambda : get_test_input_fn() )\n",
    "\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "# class_ids determine the prediction\n",
    "\n",
    "predictions = [p['class_ids'][0] for p in raw_predictions]\n",
    "#confusion_matrix = tf.confusion_matrix(df_test['accepted'], predictions)\n",
    "#print(confusion_matrix)\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run(tf.shape(predictions))) # output: [ 0.42116176  0.40666069]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_one_shot_iterator()  \n",
    "\n",
    "##### Parse all the *.csv files using make_one_shot_iterator()  \n",
    "\n",
    "##### How it works:\n",
    "\n",
    "Suppose we have only 2.csv files having #entries 10185 and 10332\n",
    "\n",
    "Total number of iterations when make_one_shot_iterator() is used:   401\n",
    "\n",
    "\n",
    "= ( 10185*10 + 10332*10 ) / 512\n",
    "\n",
    "= ( 101850 + 103320 ) / 512 =  400.72265625\n",
    "\n",
    "where \n",
    "  - 10 = 10 epochs\n",
    "  - 512 = batch size\n",
    "  - 10185*10  = 1st csv total no of entries to be iterated over\n",
    "  - 10332*10  = 2st csv total no of entries to be iterated over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-48e40be9b234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accepted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test_predictions = df_test.copy(deep = True)\n",
    "df_test_predictions['accepted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(df_test, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_test_predictions, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
