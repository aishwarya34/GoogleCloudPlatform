{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shread train-dev-test csv datasets progressively using BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max_farmhash  count\n",
      "0         None      0\n",
      "   min_farmhash10  max_farmhash10  count\n",
      "0             100             150  10333\n"
     ]
    }
   ],
   "source": [
    "def test_sample(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MAX(farmhash) as max_farmhash, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (\n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)  as farmhash, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2} )\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           a, b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "EVERY_N = 100\n",
    "query_maxhash = test_sample(0,70).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_maxhash = bq.Query(query_maxhash).execute().result().to_dataframe()\n",
    "print(df_maxhash)\n",
    "\n",
    "\n",
    "def test_sample2(a,b):\n",
    "  basequery = \"\"\"\n",
    "  SELECT MIN(farmhash10) as min_farmhash10, MAX(farmhash10) as max_farmhash10, COUNT(answer_count) as count\n",
    "  FROM\n",
    "  (  \n",
    "  SELECT \n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100)*10  as farmhash10, answer_count\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < 20 AND  MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= 10  \"\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2})\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (10*10)+a, (10*10)+b\n",
    "          )\n",
    "    \n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "  #return \"{}\\n{}\".format(basequery, sampler)\n",
    "\n",
    "\n",
    "EVERY_N = 100\n",
    "queryhash = test_sample2(0,60).replace(\"EVERY_N\", str(EVERY_N))\n",
    "df_hash = bq.Query(queryhash).execute().result().to_dataframe()\n",
    "print(df_hash.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_between(a, b, shredstart):\n",
    "  basequery = \"\"\"\n",
    "  SELECT \n",
    "    answer_count, comment_count, favorite_count,  score, view_count,\n",
    "    TIMESTAMP_DIFF(last_activity_date, creation_date, DAY) as days_posted,\n",
    "    IF(accepted_answer_id IS NULL , 0, 1) as accepted\n",
    "  FROM \n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "  \"\"\"\n",
    "  \n",
    "  # Use sampling for initial model development. Once model is developed, shread the entire dataset into  .csv files based on condition in the sampler.\n",
    "  sampler = \"WHERE MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) < {1} AND MOD(ABS(FARM_FINGERPRINT(CAST(id as STRING))), EVERY_N * 100) >= {0}\".format(\n",
    "            shredstart, shredstart + 10\n",
    "            )\n",
    "  sampler2 = \"AND {0} >= {1}\\n AND {0} < {2}\".format(\n",
    "           \"MOD(ABS(FARM_FINGERPRINT(CAST(id AS STRING))), EVERY_N * 100) * {}\".format(10),\n",
    "           (shredstart*10)+a, (shredstart*10)+b\n",
    "          )\n",
    "  return \"{}\\n{}\\n{}\".format(basequery, sampler, sampler2)\n",
    "\n",
    "\n",
    "def create_query(phase, EVERY_N, shredstart):\n",
    "  \"\"\"Phase: train (70%) valid (15%) or test (15%)\"\"\"\n",
    "  query = \"\"\n",
    "  if phase == 'train':\n",
    "    query = sample_between(0,60, shredstart)\n",
    "  elif phase == 'valid':\n",
    "    query = sample_between(60,75, shredstart)\n",
    "  else:\n",
    "    query = sample_between(75, 100, shredstart)\n",
    "  return query.replace(\"EVERY_N\", str(EVERY_N))\n",
    "\n",
    "#print(create_query('train', 100))\n",
    "#(answer_count - AVG(answer_count)) / STDDEV_POP(answer_count)  as answer_count,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "  outdf = df.copy(deep = True)\n",
    "  #outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # Reorder columns so that target is first column\n",
    "  #print(outdf.head())\n",
    "  #print(df.head())\n",
    "  cols = outdf.columns.tolist()\n",
    "  #print(cols)\n",
    "  cols.remove('accepted')\n",
    "  cols.insert(0, 'accepted')\n",
    "  #print(cols)\n",
    "  outdf = outdf[cols]  \n",
    "  \n",
    "  \n",
    "  #Normalizing input columns  and replace NaN or null\n",
    "  normalize_cols = outdf.columns.tolist()\n",
    "  normalize_cols.remove('accepted')\n",
    "  for normalize_cols_name in normalize_cols:\n",
    "    outdf[normalize_cols_name].fillna(0, inplace = True)\n",
    "    outdf[normalize_cols_name] = (outdf[normalize_cols_name] - outdf[normalize_cols_name].mean())  / outdf[normalize_cols_name].std() \n",
    "  #print(outdf)\n",
    "  #print(outdf['answer_count'] )\n",
    "  outdf.to_csv(filename,  header = True, index_label = False, index = False)\n",
    "  print(\"Wrote {} to {}\".format(len(outdf), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10186 to stackoverflow-train-10.csv\n",
      "Wrote 10333 to stackoverflow-train-20.csv\n",
      "Wrote 10426 to stackoverflow-train-30.csv\n",
      "Wrote 10260 to stackoverflow-train-40.csv\n",
      "Wrote 10298 to stackoverflow-train-50.csv\n",
      "Wrote 10401 to stackoverflow-train-60.csv\n",
      "Wrote 10276 to stackoverflow-train-70.csv\n",
      "Wrote 10249 to stackoverflow-train-80.csv\n",
      "Wrote 10291 to stackoverflow-train-90.csv\n",
      "Wrote 10332 to stackoverflow-train-100.csv\n",
      "Wrote 3500 to stackoverflow-valid-10.csv\n",
      "Wrote 3453 to stackoverflow-valid-20.csv\n",
      "Wrote 3367 to stackoverflow-valid-30.csv\n",
      "Wrote 3573 to stackoverflow-valid-40.csv\n",
      "Wrote 3482 to stackoverflow-valid-50.csv\n",
      "Wrote 3476 to stackoverflow-valid-60.csv\n",
      "Wrote 3431 to stackoverflow-valid-70.csv\n",
      "Wrote 3496 to stackoverflow-valid-80.csv\n",
      "Wrote 3469 to stackoverflow-valid-90.csv\n",
      "Wrote 3354 to stackoverflow-valid-100.csv\n",
      "Wrote 3352 to stackoverflow-test-10.csv\n",
      "Wrote 3476 to stackoverflow-test-20.csv\n",
      "Wrote 3439 to stackoverflow-test-30.csv\n",
      "Wrote 3408 to stackoverflow-test-40.csv\n",
      "Wrote 3571 to stackoverflow-test-50.csv\n",
      "Wrote 3454 to stackoverflow-test-60.csv\n",
      "Wrote 3441 to stackoverflow-test-70.csv\n",
      "Wrote 3594 to stackoverflow-test-80.csv\n",
      "Wrote 3441 to stackoverflow-test-90.csv\n",
      "Wrote 3405 to stackoverflow-test-100.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "  for x in range(10):\n",
    "    query = create_query(phase, 100, x*10)\n",
    "    #print(query)\n",
    "    df = bq.Query(query).execute().result().to_dataframe()\n",
    "    #print(df.head())\n",
    "    to_csv(df, 'stackoverflow-{}-{}.csv'.format(phase,(x+1)*10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refactor by not using sampling and creating large shreaded datasets (check size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  421033 May 16 01:32 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  414416 May 16 01:31 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  429912 May 16 01:31 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427740 May 16 01:31 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418516 May 16 01:31 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  440698 May 16 01:32 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  421966 May 16 01:32 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429984 May 16 01:32 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  444369 May 16 01:32 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427138 May 16 01:32 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1286710 May 16 01:30 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272065 May 16 01:29 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1284726 May 16 01:29 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1312253 May 16 01:29 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280413 May 16 01:29 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1279050 May 16 01:29 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1294677 May 16 01:29 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1276624 May 16 01:30 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1267578 May 16 01:30 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1283289 May 16 01:30 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  418274 May 16 01:31 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431507 May 16 01:30 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  428625 May 16 01:30 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413365 May 16 01:30 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  442074 May 16 01:30 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  428063 May 16 01:31 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  427971 May 16 01:31 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423196 May 16 01:31 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  432873 May 16 01:31 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431105 May 16 01:31 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted,answer_count,comment_count,favorite_count,score,view_count,days_posted\n",
      "0,-1.0411391480812462,-0.367090763865624,-0.16657359952844625,-0.17285273952546185,-0.19863303033934546,-0.36901565664583064\n",
      "1,0.3044971393420383,0.0008645057730129897,-0.16657359952844625,-0.08359529668588077,0.0982786543917133,-0.36901565664583064\n",
      "1,-0.3683210043696039,0.7367750450502869,-0.16657359952844625,0.005662146153700297,-0.1545800154267018,-0.07929916822667062\n",
      "1,-0.3683210043696039,-0.367090763865624,-0.16657359952844625,-0.351367625204624,0.03017964413229615,-0.36901565664583064\n",
      "1,-0.3683210043696039,0.0008645057730129897,0.4539768451043321,-0.08359529668588077,0.19286583140031063,0.33218222112228135\n",
      "1,0.3044971393420383,0.0008645057730129897,0.4539768451043321,-0.08359529668588077,0.07056940194132977,-0.35431989274051096\n",
      "0,-0.3683210043696039,-0.367090763865624,0.14370162278794293,-0.17285273952546185,-0.19534549191302877,-0.36901565664583064\n",
      "0,-1.0411391480812462,0.36881977541164995,-0.16657359952844625,-0.08359529668588077,-0.19750587430746544,-0.36901565664583064\n",
      "1,-0.3683210043696039,-0.735046033504261,-0.16657359952844625,-0.17285273952546185,-0.16660301310008857,-0.35012110305327676\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "head stackoverflow-test-100.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.estimator modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 296kB/s eta 0:00:01  2% |▉                               | 2.4MB 36.0MB/s eta 0:00:03    8% |██▋                             | 7.4MB 36.1MB/s eta 0:00:03    9% |███                             | 8.9MB 35.3MB/s eta 0:00:03    11% |███▋                            | 10.4MB 32.7MB/s eta 0:00:03    12% |████▏                           | 11.9MB 28.9MB/s eta 0:00:03    22% |███████▎                        | 21.0MB 30.4MB/s eta 0:00:03    25% |████████▎                       | 24.0MB 36.1MB/s eta 0:00:02    27% |████████▉                       | 25.5MB 35.7MB/s eta 0:00:02    48% |███████████████▌                | 44.9MB 15.5MB/s eta 0:00:04    49% |████████████████                | 46.2MB 14.5MB/s eta 0:00:04    56% |██████████████████▎             | 52.7MB 33.2MB/s eta 0:00:02    60% |███████████████████▎            | 55.6MB 35.1MB/s eta 0:00:02    61% |███████████████████▊            | 57.0MB 36.5MB/s eta 0:00:01    69% |██████████████████████▎         | 64.5MB 35.3MB/s eta 0:00:01    74% |███████████████████████▉        | 68.8MB 33.8MB/s eta 0:00:01    75% |████████████████████████▎       | 70.2MB 29.0MB/s eta 0:00:01    80% |█████████████████████████▉      | 74.6MB 36.2MB/s eta 0:00:01    85% |███████████████████████████▎    | 78.9MB 23.0MB/s eta 0:00:01    91% |█████████████████████████████▎  | 84.6MB 32.6MB/s eta 0:00:01    93% |██████████████████████████████  | 86.5MB 13.3MB/s eta 0:00:01    97% |███████████████████████████████▏| 90.0MB 16.4MB/s eta 0:00:01    98% |███████████████████████████████▌| 91.2MB 34.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.17.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.2.2)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 23.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 6.6MB/s eta 0:00:01    52% |████████████████▋               | 1.7MB 37.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 15.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.14.0)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow==1.13.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 13.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.31.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.6.11)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.14.1)\n",
      "Requirement already satisfied: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (40.2.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/envs/py3env/lib/python3.5/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.2.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, tensorflow\n",
      "  Found existing installation: tensorboard 1.8.0\n",
      "    Uninstalling tensorboard-1.8.0:\n",
      "      Successfully uninstalled tensorboard-1.8.0\n",
      "  Found existing installation: tensorflow 1.8.0\n",
      "    Uninstalling tensorflow-1.8.0:\n",
      "      Successfully uninstalled tensorflow-1.8.0\n",
      "Successfully installed keras-applications-1.0.7 keras-preprocessing-1.0.9 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
     ]
    }
   ],
   "source": [
    "# Ensure that we have TensorFlow 1.13.1 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  421033 May 16 01:32 stackoverflow-test-100.csv\r\n",
      "-rw-r--r-- 1 root root  414416 May 16 01:31 stackoverflow-test-10.csv\r\n",
      "-rw-r--r-- 1 root root  429912 May 16 01:31 stackoverflow-test-20.csv\r\n",
      "-rw-r--r-- 1 root root  427740 May 16 01:31 stackoverflow-test-30.csv\r\n",
      "-rw-r--r-- 1 root root  418516 May 16 01:31 stackoverflow-test-40.csv\r\n",
      "-rw-r--r-- 1 root root  440698 May 16 01:32 stackoverflow-test-50.csv\r\n",
      "-rw-r--r-- 1 root root  421966 May 16 01:32 stackoverflow-test-60.csv\r\n",
      "-rw-r--r-- 1 root root  429984 May 16 01:32 stackoverflow-test-70.csv\r\n",
      "-rw-r--r-- 1 root root  444369 May 16 01:32 stackoverflow-test-80.csv\r\n",
      "-rw-r--r-- 1 root root  427138 May 16 01:32 stackoverflow-test-90.csv\r\n",
      "-rw-r--r-- 1 root root 1286710 May 16 01:30 stackoverflow-train-100.csv\r\n",
      "-rw-r--r-- 1 root root 1272065 May 16 01:29 stackoverflow-train-10.csv\r\n",
      "-rw-r--r-- 1 root root 1284726 May 16 01:29 stackoverflow-train-20.csv\r\n",
      "-rw-r--r-- 1 root root 1312253 May 16 01:29 stackoverflow-train-30.csv\r\n",
      "-rw-r--r-- 1 root root 1280413 May 16 01:29 stackoverflow-train-40.csv\r\n",
      "-rw-r--r-- 1 root root 1279050 May 16 01:29 stackoverflow-train-50.csv\r\n",
      "-rw-r--r-- 1 root root 1294677 May 16 01:29 stackoverflow-train-60.csv\r\n",
      "-rw-r--r-- 1 root root 1276624 May 16 01:30 stackoverflow-train-70.csv\r\n",
      "-rw-r--r-- 1 root root 1267578 May 16 01:30 stackoverflow-train-80.csv\r\n",
      "-rw-r--r-- 1 root root 1283289 May 16 01:30 stackoverflow-train-90.csv\r\n",
      "-rw-r--r-- 1 root root  418274 May 16 01:31 stackoverflow-valid-100.csv\r\n",
      "-rw-r--r-- 1 root root  431507 May 16 01:30 stackoverflow-valid-10.csv\r\n",
      "-rw-r--r-- 1 root root  428625 May 16 01:30 stackoverflow-valid-20.csv\r\n",
      "-rw-r--r-- 1 root root  413365 May 16 01:30 stackoverflow-valid-30.csv\r\n",
      "-rw-r--r-- 1 root root  442074 May 16 01:30 stackoverflow-valid-40.csv\r\n",
      "-rw-r--r-- 1 root root  428063 May 16 01:31 stackoverflow-valid-50.csv\r\n",
      "-rw-r--r-- 1 root root  427971 May 16 01:31 stackoverflow-valid-60.csv\r\n",
      "-rw-r--r-- 1 root root  423196 May 16 01:31 stackoverflow-valid-70.csv\r\n",
      "-rw-r--r-- 1 root root  432873 May 16 01:31 stackoverflow-valid-80.csv\r\n",
      "-rw-r--r-- 1 root root  431105 May 16 01:31 stackoverflow-valid-90.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Before reading csv was incorporated using pandas dataframe\n",
    "# But now reading csv is incorporated using tensorflow and so it's in the graps and also it reads progressively the shreaded files\n",
    "\n",
    "df_train = pd.read_csv(filepath_or_buffer = \"./stackoverflow-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./stackoverflow-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./stackoverflow-test.csv\")\n",
    "\n",
    "CSV_COLUMNNAMES = list(df_train) # CSV_COLUMNNAMES = df_train.columns.tolist()\n",
    "print(CSV_COLUMNNAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMNNAMES[1:]\n",
    "LABEL_NAME = CSV_COLUMNNAMES[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['accepted', 'answer_count', 'comment_count', 'favorite_count', 'score', 'view_count', 'days_posted']\n",
    "DEFAULTS = [[0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "DEFAULTS = [tf.constant([], dtype=tf.int32),\n",
    "            tf.constant([], dtype=tf.float32)],\n",
    "\n",
    "            ]\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def decode_line(row):\n",
    "    cols = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    #print(cols)\n",
    "    features = dict(zip(CSV_COLUMNS,cols))\n",
    "    #print(cols)\n",
    "    label = features.pop('accepted')  # remove label from features and store\n",
    "    return features, label\n",
    "  \n",
    "  # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "  # Read lines from text files\n",
    "  textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "  # Parse text lines as comma-separated values (CSV)\n",
    "  dataset = textlines_dataset.map(decode_line)\n",
    "  \n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "  \n",
    "  if(mode == tf.estimator.ModeKeys.TRAIN):\n",
    "    num_epochs = None  # loop indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size = 10*batch_size, seed=2)\n",
    "  else:\n",
    "    num_epochs = 1\n",
    "  \n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "  \n",
    "def get_train_input_fn():\n",
    "  return read_dataset('./stackoverflow-train-10.csv', tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "  return read_dataset('./stackoverflow-valid-10.csv', tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "def get_test_input_fn():\n",
    "  return read_dataset('./stackoverflow-test-10.csv', tf.estimator.ModeKeys.PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = CSV_COLUMNS[1:]\n",
    "LABEL_NAME = CSV_COLUMNS[0]\n",
    "\n",
    "featcols = [ tf.feature_column.numeric_column(feat) for feat in  FEATURE_NAMES ]\n",
    "#print(featcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_eval_distribute': None, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_task_type': 'worker', '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83d0829358>, '_model_dir': 'stackoverflow_model', '_num_ps_replicas': 0, '_protocol': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_master': '', '_global_id_in_cluster': 0, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_is_chief': True, '_task_id': 0, '_log_step_count_steps': 100, '_device_fn': None, '_save_checkpoints_steps': None}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into stackoverflow_model/model.ckpt.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Field 0 in record 0 is not a valid int32: accepted\n\t [[{{node DecodeCSV}}]]\n\t [[node IteratorGetNext (defined at /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/util.py:110) ]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Field 0 in record 0 is not a valid int32: accepted\n\t [[{{node DecodeCSV}}]]\n\t [[{{node IteratorGetNext}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1157\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1405\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Field 0 in record 0 is not a valid int32: accepted\n\t [[{{node DecodeCSV}}]]\n\t [[node IteratorGetNext (defined at /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/util.py:110) ]]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = \"stackoverflow_model\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [1024, 512, 128, 32],  # specify neural architecture\n",
    "    feature_columns = featcols,\n",
    "    n_classes=2,\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)  \n",
    "  )\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda : get_train_input_fn(),\n",
    "    steps = 200\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_rmse(model, df_validation):\n",
    "  metrices = model.evaluate(input_fn = lambda : get_valid_input_fn() )\n",
    "  print(\"RMSE on dataset = {}\".format(metrices[\"average_loss\"]**.5))\n",
    "\n",
    "#validate_rmse(model, df_train)\n",
    "validate_rmse(model, df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on training set evaluate\n",
    "\n",
    "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.6811679, accuracy_baseline = 0.5281519, auc = 0.7331483, auc_precision_recall = 0.7117538, average_loss = 0.56418276, global_step = 500, label/mean = 0.5281519, loss = 71.741234, precision = 0.6524123, prediction/mean = 0.48095724, recall = 0.8482496\n",
    "RMSE on dataset = 0.7511210011251841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = model.predict( input_fn = lambda : get_test_input_fn() )\n",
    "\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "#print(next(raw_predictions))\n",
    "# class_ids determine the prediction\n",
    "\n",
    "predictions = [p['class_ids'][0] for p in raw_predictions]\n",
    "\n",
    "#confusion_matrix = tf.confusion_matrix(df_test['accepted'], predictions)\n",
    "#print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predictions = df_test.copy(deep = True)\n",
    "df_test_predictions['accepted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(df_test, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_test_predictions, hue=\"accepted\", palette=\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
